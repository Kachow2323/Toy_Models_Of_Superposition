{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Toy Models of Superposition\n",
        "\n",
        "This notebook includes the toy model training framework used to generate most of the results in the \"Toy Models of Superposition\" paper.\n",
        "\n",
        "The main useful improvement over a basic PyTorch tiny autoencoder is the ability to batch train many models with varying sparsity at once, which is much more efficient than training them one at a time.\n",
        "\n",
        "This notebook is designed to run in Google Colab's Python 3.7 environment."
      ],
      "metadata": {
        "id": "SifuSeuwi6nj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfHPSGevYsdo",
        "outputId": "79d18c6c-a268-4ac1-cebc-37abc6be346b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRLgezMAZbtY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "from dataclasses import dataclass, replace\n",
        "import numpy as np\n",
        "import einops\n",
        "\n",
        "from tqdm.notebook import trange\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upSmPXMrZct4"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  n_features: int\n",
        "  n_hidden: int\n",
        "\n",
        "  # We optimize n_instances models in a single training loop\n",
        "  # to let us sweep over sparsity or importance curves\n",
        "  # efficiently.\n",
        "\n",
        "  # We could potentially use torch.vmap instead.\n",
        "  n_instances: int\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self,\n",
        "               config,\n",
        "               feature_probability: Optional[torch.Tensor] = None,\n",
        "               importance: Optional[torch.Tensor] = None,\n",
        "               device='cuda'):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.W = nn.Parameter(torch.empty((config.n_instances, config.n_features, config.n_hidden), device=device))\n",
        "    nn.init.xavier_normal_(self.W)\n",
        "    self.b_final = nn.Parameter(torch.zeros((config.n_instances, config.n_features), device=device))\n",
        "\n",
        "    if feature_probability is None:\n",
        "      feature_probability = torch.ones(())\n",
        "    self.feature_probability = feature_probability.to(device)\n",
        "    if importance is None:\n",
        "      importance = torch.ones(())\n",
        "    self.importance = importance.to(device)\n",
        "\n",
        "  def forward(self, features):\n",
        "    # features: [..., instance, n_features]\n",
        "    # W: [instance, n_features, n_hidden]\n",
        "    hidden = torch.einsum(\"...if,ifh->...ih\", features, self.W)\n",
        "    out = torch.einsum(\"...ih,ifh->...if\", hidden, self.W)\n",
        "    out = out + self.b_final\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "  def generate_batch(self, n_batch):\n",
        "    feat = torch.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device)\n",
        "    batch = torch.where(\n",
        "        torch.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device) <= self.feature_probability,\n",
        "        feat,\n",
        "        torch.zeros((), device=self.W.device),\n",
        "    )\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ey4ApHYb9oY"
      },
      "outputs": [],
      "source": [
        "def linear_lr(step, steps):\n",
        "  return (1 - (step / steps))\n",
        "\n",
        "def constant_lr(*_):\n",
        "  return 1.0\n",
        "\n",
        "def cosine_decay_lr(step, steps):\n",
        "  return np.cos(0.5 * np.pi * step / (steps - 1))\n",
        "\n",
        "def optimize(model,\n",
        "             render=False,\n",
        "             n_batch=1024,\n",
        "             steps=10_000,\n",
        "             print_freq=100,\n",
        "             lr=1e-3,\n",
        "             lr_scale=constant_lr,\n",
        "             hooks=[]):\n",
        "  cfg = model.config\n",
        "\n",
        "  opt = torch.optim.AdamW(list(model.parameters()), lr=lr)\n",
        "\n",
        "  start = time.time()\n",
        "  with trange(steps) as t:\n",
        "    for step in t:\n",
        "      step_lr = lr * lr_scale(step, steps)\n",
        "      for group in opt.param_groups:\n",
        "        group['lr'] = step_lr\n",
        "      opt.zero_grad(set_to_none=True)\n",
        "      batch = model.generate_batch(n_batch)\n",
        "      out = model(batch)\n",
        "      error = (model.importance*(batch.abs() - out)**2)\n",
        "      loss = einops.reduce(error, 'b i f -> i', 'mean').sum()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      if hooks:\n",
        "        hook_data = dict(model=model,\n",
        "                         step=step,\n",
        "                         opt=opt,\n",
        "                         error=error,\n",
        "                         loss=loss,\n",
        "                         lr=step_lr)\n",
        "        for h in hooks:\n",
        "          h(hook_data)\n",
        "      if step % print_freq == 0 or (step + 1 == steps):\n",
        "        t.set_postfix(\n",
        "            loss=loss.item() / cfg.n_instances,\n",
        "            lr=step_lr,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsIgAC56dWMk"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  DEVICE = 'cuda'\n",
        "else:\n",
        "  DEVICE = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction Figure\n",
        "\n",
        "Reproducing a version of the figure from the introduction, although with a slightly different version of the code."
      ],
      "metadata": {
        "id": "QpshP3ppjsIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(\n",
        "    n_features = 5,\n",
        "    n_hidden = 2,\n",
        "    n_instances = 10,\n",
        ")\n",
        "\n",
        "model = Model(\n",
        "    config=config,\n",
        "    device=DEVICE,\n",
        "    # Exponential feature importance curve from 1 to 1/100\n",
        "    importance = (0.9**torch.arange(config.n_features))[None, :],\n",
        "    # Sweep feature frequency across the instances from 1 (fully dense) to 1/20\n",
        "    feature_probability = (20 ** -torch.linspace(0, 1, config.n_instances))[:, None]\n",
        ")"
      ],
      "metadata": {
        "id": "crVhVxhIj5pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimize(model)"
      ],
      "metadata": {
        "id": "oveRpg0skeJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_intro_diagram(model):\n",
        "  from matplotlib import colors  as mcolors\n",
        "  from matplotlib import collections  as mc\n",
        "  cfg = model.config\n",
        "  WA = model.W.detach()\n",
        "  N = len(WA[:,0])\n",
        "  sel = range(config.n_instances) # can be used to highlight specific sparsity levels\n",
        "  plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\"color\", plt.cm.viridis(model.importance[0].cpu().numpy()))\n",
        "  plt.rcParams['figure.dpi'] = 200\n",
        "  fig, axs = plt.subplots(1,len(sel), figsize=(2*len(sel),2))\n",
        "  for i, ax in zip(sel, axs):\n",
        "      W = WA[i].cpu().detach().numpy()\n",
        "      colors = [mcolors.to_rgba(c)\n",
        "            for c in plt.rcParams['axes.prop_cycle'].by_key()['color']]\n",
        "      ax.scatter(W[:,0], W[:,1], c=colors[0:len(W[:,0])])\n",
        "      ax.set_aspect('equal')\n",
        "      ax.add_collection(mc.LineCollection(np.stack((np.zeros_like(W),W), axis=1), colors=colors))\n",
        "\n",
        "      z = 1.5\n",
        "      ax.set_facecolor('#FCFBF8')\n",
        "      ax.set_xlim((-z,z))\n",
        "      ax.set_ylim((-z,z))\n",
        "      ax.tick_params(left = True, right = False , labelleft = False ,\n",
        "                  labelbottom = False, bottom = True)\n",
        "      for spine in ['top', 'right']:\n",
        "          ax.spines[spine].set_visible(False)\n",
        "      for spine in ['bottom','left']:\n",
        "          ax.spines[spine].set_position('center')\n",
        "  plt.show()\n",
        "\n",
        "plot_intro_diagram(model)"
      ],
      "metadata": {
        "id": "hAREurbUkji1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing features across varying sparsity"
      ],
      "metadata": {
        "id": "0px3BWa3ALjo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG-yEYencTIo"
      },
      "outputs": [],
      "source": [
        "config = Config(\n",
        "    n_features = 100,\n",
        "    n_hidden = 20,\n",
        "    n_instances = 20,\n",
        ")\n",
        "\n",
        "model = Model(\n",
        "    config=config,\n",
        "    device=DEVICE,\n",
        "    # Exponential feature importance curve from 1 to 1/100\n",
        "    importance = (100 ** -torch.linspace(0, 1, config.n_features))[None, :],\n",
        "    # Sweep feature frequency across the instances from 1 (fully dense) to 1/20\n",
        "    feature_probability = (20 ** -torch.linspace(0, 1, config.n_instances))[:, None]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po6L0a-5dUkZ"
      },
      "outputs": [],
      "source": [
        "optimize(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpatvhlHfEyC"
      },
      "outputs": [],
      "source": [
        "def render_features(model, which=np.s_[:]):\n",
        "  cfg = model.config\n",
        "  W = model.W.detach()\n",
        "  W_norm = W / (1e-5 + torch.linalg.norm(W, 2, dim=-1, keepdim=True))\n",
        "\n",
        "  interference = torch.einsum('ifh,igh->ifg', W_norm, W)\n",
        "  interference[:, torch.arange(cfg.n_features), torch.arange(cfg.n_features)] = 0\n",
        "\n",
        "  polysemanticity = torch.linalg.norm(interference, dim=-1).cpu()\n",
        "  net_interference = (interference**2 * model.feature_probability[:, None, :]).sum(-1).cpu()\n",
        "  norms = torch.linalg.norm(W, 2, dim=-1).cpu()\n",
        "\n",
        "  WtW = torch.einsum('sih,soh->sio', W, W).cpu()\n",
        "\n",
        "  # width = weights[0].cpu()\n",
        "  # x = torch.cumsum(width+0.1, 0) - width[0]\n",
        "  x = torch.arange(cfg.n_features)\n",
        "  width = 0.9\n",
        "\n",
        "  which_instances = np.arange(cfg.n_instances)[which]\n",
        "  fig = make_subplots(rows=len(which_instances),\n",
        "                      cols=2,\n",
        "                      shared_xaxes=True,\n",
        "                      vertical_spacing=0.02,\n",
        "                      horizontal_spacing=0.1)\n",
        "  for (row, inst) in enumerate(which_instances):\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=x,\n",
        "              y=norms[inst],\n",
        "              marker=dict(\n",
        "                  color=polysemanticity[inst],\n",
        "                  cmin=0,\n",
        "                  cmax=1\n",
        "              ),\n",
        "              width=width,\n",
        "        ),\n",
        "        row=1+row, col=1\n",
        "    )\n",
        "    data = WtW[inst].numpy()\n",
        "    fig.add_trace(\n",
        "        go.Image(\n",
        "            z=plt.cm.coolwarm((1 + data)/2, bytes=True),\n",
        "            colormodel='rgba256',\n",
        "            customdata=data,\n",
        "            hovertemplate='''\\\n",
        "In: %{x}<br>\n",
        "Out: %{y}<br>\n",
        "Weight: %{customdata:0.2f}\n",
        "'''\n",
        "        ),\n",
        "        row=1+row, col=2\n",
        "    )\n",
        "\n",
        "  fig.add_vline(\n",
        "    x=(x[cfg.n_hidden-1]+x[cfg.n_hidden])/2,\n",
        "    line=dict(width=0.5),\n",
        "    col=1,\n",
        "  )\n",
        "\n",
        "  # fig.update_traces(marker_size=1)\n",
        "  fig.update_layout(showlegend=False,\n",
        "                    width=600,\n",
        "                    height=100*len(which_instances),\n",
        "                    margin=dict(t=0, b=0))\n",
        "  fig.update_xaxes(visible=False)\n",
        "  fig.update_yaxes(visible=False)\n",
        "  return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm3YV12Hfpvg"
      },
      "outputs": [],
      "source": [
        "fig = render_features(model, np.s_[::2])\n",
        "fig.update_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature geometry"
      ],
      "metadata": {
        "id": "qDIgjx2GAQNx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECrtk9nJARlX"
      },
      "outputs": [],
      "source": [
        "config = Config(\n",
        "    n_features = 200,\n",
        "    n_hidden = 20,\n",
        "    n_instances = 20,\n",
        ")\n",
        "\n",
        "model = Model(\n",
        "    config=config,\n",
        "    device=DEVICE,\n",
        "    # For this experiment, use constant importance.\n",
        "\n",
        "    # Sweep feature frequency across the instances from 1 (fully dense) to 1/20\n",
        "    feature_probability = (20 ** -torch.linspace(0, 1, config.n_instances))[:, None]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DNkKnDKARlY"
      },
      "outputs": [],
      "source": [
        "optimize(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.line(\n",
        "    x=1/model.feature_probability[:, 0].cpu(),\n",
        "    y=(model.config.n_hidden/(torch.linalg.matrix_norm(model.W.detach(), 'fro')**2)).cpu(),\n",
        "    log_x=True,\n",
        "    markers=True,\n",
        ")\n",
        "fig.update_xaxes(title=\"1/(1-S)\")\n",
        "fig.update_yaxes(title=f\"m/||W||_F^2\")"
      ],
      "metadata": {
        "id": "qFPKm2ObAfre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def compute_dimensionality(W):\n",
        "  norms = torch.linalg.norm(W, 2, dim=-1)\n",
        "  W_unit = W / torch.clamp(norms[:, :, None], 1e-6, float('inf'))\n",
        "\n",
        "  interferences = (torch.einsum('eah,ebh->eab', W_unit, W)**2).sum(-1)\n",
        "\n",
        "  dim_fracs = (norms**2/interferences)\n",
        "  return dim_fracs.cpu()"
      ],
      "metadata": {
        "id": "m1gNMKCqA6EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim_fracs = compute_dimensionality(model.W)"
      ],
      "metadata": {
        "id": "3OfjWWC9BDlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "density = model.feature_probability[:, 0].cpu()\n",
        "W = model.W.detach()\n",
        "\n",
        "for a,b in [(1,2), (2,3), (2,5), (2,6), (2,7)]:\n",
        "    val = a/b\n",
        "    fig.add_hline(val, line_color=\"purple\", opacity=0.2, annotation=dict(text=f\"{a}/{b}\"))\n",
        "\n",
        "for a,b in [(5,6), (4,5), (3,4), (3,8), (3,12), (3,20)]:\n",
        "    val = a/b\n",
        "    fig.add_hline(val, line_color=\"blue\", opacity=0.2, annotation=dict(text=f\"{a}/{b}\", x=0.05))\n",
        "\n",
        "for i in range(len(W)):\n",
        "    fracs_ = dim_fracs[i]\n",
        "    N = fracs_.shape[0]\n",
        "    xs = 1/density\n",
        "    if i!= len(W)-1:\n",
        "        dx = xs[i+1]-xs[i]\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=1/density[i]*np.ones(N)+dx*np.random.uniform(-0.1,0.1,N),\n",
        "            y=fracs_,\n",
        "            marker=dict(\n",
        "                color='black',\n",
        "                size=1,\n",
        "                opacity=0.5,\n",
        "            ),\n",
        "            mode='markers',\n",
        "        )\n",
        "    )\n",
        "\n",
        "fig.update_xaxes(\n",
        "    type='log',\n",
        "    title='1/(1-S)',\n",
        "    showgrid=False,\n",
        ")\n",
        "fig.update_yaxes(\n",
        "    showgrid=False\n",
        ")\n",
        "fig.update_layout(showlegend=False)"
      ],
      "metadata": {
        "id": "DTWEr8pHBGCf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}